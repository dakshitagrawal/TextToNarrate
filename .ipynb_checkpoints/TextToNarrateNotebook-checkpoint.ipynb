{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "C:\\Users\\shyam\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Dropout, TimeDistributed\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk import FreqDist\n",
    "import gensim\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>war</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demand</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>British</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>troops</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>from</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>that</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>country</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Families</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>soldiers</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>killed</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048545</th>\n",
       "      <td>Sentence: 47957</td>\n",
       "      <td>Two</td>\n",
       "      <td>CD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048546</th>\n",
       "      <td>NaN</td>\n",
       "      <td>more</td>\n",
       "      <td>JJR</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048547</th>\n",
       "      <td>NaN</td>\n",
       "      <td>landed</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048548</th>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048549</th>\n",
       "      <td>NaN</td>\n",
       "      <td>fields</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048550</th>\n",
       "      <td>NaN</td>\n",
       "      <td>belonging</td>\n",
       "      <td>VBG</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048551</th>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048552</th>\n",
       "      <td>NaN</td>\n",
       "      <td>a</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048553</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nearby</td>\n",
       "      <td>JJ</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048554</th>\n",
       "      <td>NaN</td>\n",
       "      <td>village</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048555</th>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048556</th>\n",
       "      <td>Sentence: 47958</td>\n",
       "      <td>They</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048557</th>\n",
       "      <td>NaN</td>\n",
       "      <td>say</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048558</th>\n",
       "      <td>NaN</td>\n",
       "      <td>not</td>\n",
       "      <td>RB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048559</th>\n",
       "      <td>NaN</td>\n",
       "      <td>all</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048560</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048561</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048562</th>\n",
       "      <td>NaN</td>\n",
       "      <td>rockets</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048563</th>\n",
       "      <td>NaN</td>\n",
       "      <td>exploded</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048564</th>\n",
       "      <td>NaN</td>\n",
       "      <td>upon</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048565</th>\n",
       "      <td>NaN</td>\n",
       "      <td>impact</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048566</th>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048567</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>Indian</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048568</th>\n",
       "      <td>NaN</td>\n",
       "      <td>forces</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048569</th>\n",
       "      <td>NaN</td>\n",
       "      <td>said</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>NaN</td>\n",
       "      <td>they</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>NaN</td>\n",
       "      <td>responded</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048572</th>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048573</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048574</th>\n",
       "      <td>NaN</td>\n",
       "      <td>attack</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1048575 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Sentence #           Word  POS    Tag\n",
       "0            Sentence: 1      Thousands  NNS      O\n",
       "1                    NaN             of   IN      O\n",
       "2                    NaN  demonstrators  NNS      O\n",
       "3                    NaN           have  VBP      O\n",
       "4                    NaN        marched  VBN      O\n",
       "5                    NaN        through   IN      O\n",
       "6                    NaN         London  NNP  B-geo\n",
       "7                    NaN             to   TO      O\n",
       "8                    NaN        protest   VB      O\n",
       "9                    NaN            the   DT      O\n",
       "10                   NaN            war   NN      O\n",
       "11                   NaN             in   IN      O\n",
       "12                   NaN           Iraq  NNP  B-geo\n",
       "13                   NaN            and   CC      O\n",
       "14                   NaN         demand   VB      O\n",
       "15                   NaN            the   DT      O\n",
       "16                   NaN     withdrawal   NN      O\n",
       "17                   NaN             of   IN      O\n",
       "18                   NaN        British   JJ  B-gpe\n",
       "19                   NaN         troops  NNS      O\n",
       "20                   NaN           from   IN      O\n",
       "21                   NaN           that   DT      O\n",
       "22                   NaN        country   NN      O\n",
       "23                   NaN              .    .      O\n",
       "24           Sentence: 2       Families  NNS      O\n",
       "25                   NaN             of   IN      O\n",
       "26                   NaN       soldiers  NNS      O\n",
       "27                   NaN         killed  VBN      O\n",
       "28                   NaN             in   IN      O\n",
       "29                   NaN            the   DT      O\n",
       "...                  ...            ...  ...    ...\n",
       "1048545  Sentence: 47957            Two   CD      O\n",
       "1048546              NaN           more  JJR      O\n",
       "1048547              NaN         landed  VBD      O\n",
       "1048548              NaN             in   IN      O\n",
       "1048549              NaN         fields  NNS      O\n",
       "1048550              NaN      belonging  VBG      O\n",
       "1048551              NaN             to   TO      O\n",
       "1048552              NaN              a   DT      O\n",
       "1048553              NaN         nearby   JJ      O\n",
       "1048554              NaN        village   NN      O\n",
       "1048555              NaN              .    .      O\n",
       "1048556  Sentence: 47958           They  PRP      O\n",
       "1048557              NaN            say  VBP      O\n",
       "1048558              NaN            not   RB      O\n",
       "1048559              NaN            all   DT      O\n",
       "1048560              NaN             of   IN      O\n",
       "1048561              NaN            the   DT      O\n",
       "1048562              NaN        rockets  NNS      O\n",
       "1048563              NaN       exploded  VBD      O\n",
       "1048564              NaN           upon   IN      O\n",
       "1048565              NaN         impact   NN      O\n",
       "1048566              NaN              .    .      O\n",
       "1048567  Sentence: 47959         Indian   JJ  B-gpe\n",
       "1048568              NaN         forces  NNS      O\n",
       "1048569              NaN           said  VBD      O\n",
       "1048570              NaN           they  PRP      O\n",
       "1048571              NaN      responded  VBD      O\n",
       "1048572              NaN             to   TO      O\n",
       "1048573              NaN            the   DT      O\n",
       "1048574              NaN         attack   NN      O\n",
       "\n",
       "[1048575 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ner_dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txt = \"\"\n",
    "txt += df.iloc[0,df.columns.get_loc('Word')]\n",
    "\n",
    "for i in xrange(1, df.shape[0]-1):\n",
    "        txt += \" \"\n",
    "        txt += df.iloc[i,df.columns.get_loc('Word')]\n",
    "\n",
    "with open(\"Story.txt\", \"w\") as text_file:\n",
    "    text_file.write(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6053792\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "no_of_datapoints = 1000\n",
    "data = open(\"Story.txt\", \"r\").read()\n",
    "charactersList = list(set(data))\n",
    "print len(data)\n",
    "print len(charactersList)\n",
    "\n",
    "char_to_index = {char:index for index,char in enumerate(charactersList)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "[14, 18, 46, 96, 47, 91, 69, 17, 47]\n"
     ]
    }
   ],
   "source": [
    "characterOneHotList = []\n",
    "\n",
    "for i in xrange(0, no_of_datapoints):\n",
    "    chars = list(df.iloc[i, df.columns.get_loc('Word')])\n",
    "    \n",
    "    char_oneHot = []\n",
    "    \n",
    "    for j in xrange(0, len(chars)):\n",
    "        char_oneHot.append(char_to_index[chars[j]])\n",
    "    \n",
    "    characterOneHotList.append(char_oneHot)\n",
    "\n",
    "print len(characterOneHotList)\n",
    "print characterOneHotList[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 17)\n"
     ]
    }
   ],
   "source": [
    "characterOneHotListPadded = pad_sequences(characterOneHotList)\n",
    "\n",
    "print characterOneHotListPadded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42,)\n"
     ]
    }
   ],
   "source": [
    "sentence_character_list = []\n",
    "sentence_character = [characterOneHotListPadded[0]]\n",
    "\n",
    "for i in xrange(1, no_of_datapoints):\n",
    "    if df['Sentence #'].isnull()[i] == True:\n",
    "        sentence_character.append(characterOneHotListPadded[i])\n",
    "    else: \n",
    "        sentence_character_list.append(sentence_character)\n",
    "        sentence_character = []\n",
    "        sentence_character.append(characterOneHotListPadded[i])\n",
    "\n",
    "sentence_character_list = np.array(sentence_character_list)\n",
    "\n",
    "print sentence_character_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "(42, 40, 17)\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 14 18 46 96 47 91 69 17 47]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 46 66]\n",
      " [ 0  0  0  0 17 92 94 46 69 47 22 70 91 22 46 70 47]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 18 91 71 92]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 94 91 70 43 18 92 17]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 22 18 70 46 96 44 18]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 12 46 69 17 46 69]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 22 46]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 21 70 46 22 92 47 22]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 22 18 92]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 48 91 70]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 93 69]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 85 70 91 95]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 91 69 17]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 17 92 94 91 69 17]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 22 18 92]\n",
      " [ 0  0  0  0  0  0  0 48 93 22 18 17 70 91 48 91 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 46 66]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 58 70 93 22 93 47 18]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 22 70 46 46 21 47]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 66 70 46 94]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 22 18 91 22]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 43 46 96 69 22 70 98]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 54]]\n"
     ]
    }
   ],
   "source": [
    "print len(sentence_character_list[1])\n",
    "sentence_character_list_padded = pad_sequences(sentence_character_list)\n",
    "print sentence_character_list_padded.shape\n",
    "print sentence_character_list_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "477\n"
     ]
    }
   ],
   "source": [
    "freqdist = FreqDist(df.iloc[:no_of_datapoints, df.columns.get_loc('Word')])\n",
    "\n",
    "vocab = []\n",
    "for key in freqdist:\n",
    "    if key.isalpha():\n",
    "        vocab.append(key)\n",
    "\n",
    "vocab.append('UNKNOWN')\n",
    "print len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[429, 416, 404, 216, 351, 122, 393, 9, 2, 360, 435, 92, 197, 213, 317, 360, 270, 416, 107, 61, 60, 200, 298, 476], [457, 416, 247, 377, 92, 360, 345, 151, 360, 386, 230, 414, 474, 439, 184, 38, 454, 476, 333, 321, 115, 427, 476, 213, 476, 339, 360, 399, 476, 476], [447, 351, 60, 360, 64, 416, 108, 9, 112, 456, 92, 177, 285, 476], [90, 169, 360, 181, 416, 67, 455, 476, 325, 421, 335, 332, 434, 476, 476], [233, 2, 305, 413, 360, 17, 416, 360, 300, 310, 416, 445, 476, 446, 365, 94, 92, 360, 106, 40, 395, 131, 416, 70, 476], [233, 344, 331, 98, 121, 445, 476, 405, 92, 360, 197, 345, 213, 360, 14, 388, 416, 476, 107, 61, 92, 200, 298, 476], [233, 393, 382, 241, 42, 416, 476, 423, 69, 92, 462, 295, 476, 271, 137, 476, 243, 476, 213, 172, 476], [233, 401, 211, 43, 244, 331, 9, 80, 25, 144, 416, 3, 92, 402, 76, 413, 100, 9, 273, 9, 188, 476, 453, 416, 476, 459, 219, 476], [188, 18, 346, 442, 342, 416, 360, 219, 91, 455, 125, 280, 284, 103, 476], [12, 278, 215, 139, 155, 9, 263, 29, 9, 433, 302, 342, 416, 360, 103, 76, 476, 105, 452, 357, 340, 190, 205, 27, 476], [233, 411, 323, 117, 360, 424, 9, 118, 455, 35, 283, 476], [233, 248, 308, 476, 439, 476, 330, 476, 381, 36, 9, 406, 188, 9, 360, 476, 171, 148, 476, 225, 173, 174, 82, 336, 332, 304, 287, 381, 276, 360, 449, 476, 237, 476], [188, 476, 31, 356, 19, 180, 282, 328, 200, 248, 166, 53, 455, 175, 188, 9, 97, 125, 284, 258, 10, 279, 452, 63, 9, 360, 12, 136, 476], [425, 226, 213, 1, 337, 154, 375, 212, 41, 412, 59, 467, 266, 112, 349, 413, 112, 309, 92, 57, 476, 106, 476, 196, 170, 476], [473, 443, 439, 360, 83, 315, 183, 206, 476, 15, 341, 476, 282, 360, 55, 147, 360, 306, 309, 76, 454, 332, 26, 60, 196, 208, 9, 296, 208, 9, 410, 452, 101, 154, 460, 182, 412, 476, 127, 476], [233, 83, 315, 394, 454, 112, 476, 403, 127, 476], [152, 39, 354, 194, 154, 120, 92, 360, 179, 196, 9, 317, 46, 232, 213, 130, 291, 8, 60, 33, 16, 476], [249, 49, 396, 88, 139, 216, 314, 469, 157, 416, 360, 96, 207, 224, 60, 193, 319, 422, 476, 272, 360, 316, 416, 57, 476, 476, 165, 149, 416, 62, 279, 290, 472, 476], [66, 245, 242, 216, 150, 87, 431, 455, 360, 214, 348, 412, 400, 476, 368, 356, 222, 253, 265, 476], [358, 434, 141, 389, 236, 336, 360, 109, 434, 92, 360, 214, 92, 143, 364, 360, 194, 466, 420, 336, 192, 434, 77, 476], [186, 294, 44, 282, 455, 191, 387, 87, 431, 262, 360, 214, 140, 213, 462, 178, 212, 150, 437, 92, 143, 76, 476], [233, 415, 466, 105, 360, 384, 282, 332, 323, 24, 42, 439, 112, 28, 310, 9, 225, 448, 204, 476, 334, 385, 476, 409, 213, 461, 279, 299, 476], [426, 48, 278, 215, 13, 213, 61, 216, 352, 92, 360, 252, 187, 303, 403, 112, 31, 254, 301, 450, 201, 92, 197, 367, 476], [210, 323, 141, 215, 100, 327, 61, 216, 352, 92, 360, 198, 223, 213, 463, 187, 476, 272, 256, 297, 346, 377, 455, 191, 476, 397, 213, 238, 448, 204, 476, 476], [476, 392, 216, 141, 52, 100, 78, 383, 323, 246, 92, 360, 254, 476], [210, 215, 450, 201, 92, 197, 367, 216, 162, 74, 167, 301, 133, 92, 114, 227, 213, 378, 9, 462, 252, 311, 476], [303, 331, 360, 343, 187, 436, 416, 378, 213, 381, 432, 314, 112, 199, 416, 198, 132, 367, 476], [359, 462, 417, 476, 476, 278, 282, 142, 78, 45, 434, 377, 325, 413, 56, 92, 378, 268, 476], [221, 128, 216, 146, 455, 191, 476, 168, 416, 360, 293, 51, 134, 454, 342, 416, 360, 298, 430, 403, 95, 47, 23, 240, 476], [233, 366, 466, 376, 92, 391, 476], [102, 250, 403, 360, 134, 282, 360, 366, 279, 452, 113, 9, 372, 360, 134, 202, 60, 125, 72, 213, 50, 403, 129, 95, 369, 92, 163, 23, 476], [233, 51, 134, 381, 292, 125, 220, 92, 71, 92, 355, 23, 476, 440, 360, 344, 476, 58, 9, 476, 369, 476], [359, 240, 476, 23, 476, 329, 323, 324, 380, 92, 86, 311, 272, 458, 373, 286, 112, 374, 92, 360, 371, 20, 416, 239, 476], [233, 51, 134, 331, 264, 454, 112, 313, 344, 476, 438, 332, 362, 476, 347, 257, 6, 307, 9, 360, 344, 331, 79, 9, 329, 476], [68, 11, 92, 390, 476, 123, 398, 0, 34, 216, 277, 122, 112, 110, 200, 209, 9, 418, 476, 407, 164, 476, 92, 32, 441, 213, 30, 112, 234, 128, 9, 176, 312, 326, 476], [102, 476, 84, 416, 5, 251, 428, 476, 360, 65, 476, 476, 158, 360, 104, 408, 476, 116, 360, 320, 434, 145, 195, 451, 412, 112, 185, 416, 476, 476], [233, 104, 322, 81, 124, 203, 360, 320, 126, 332, 75, 110, 476, 112, 411, 217, 255, 454, 112, 73, 476], [233, 363, 110, 470, 403, 379, 444, 112, 476, 5, 128, 21, 476, 9, 338, 228, 397, 160, 9, 407, 471, 92, 32, 441, 476, 213, 159, 465, 281, 266, 288, 376, 275, 476], [229, 173, 22, 259, 403, 444, 9, 99, 89, 476], [233, 293, 381, 135, 360, 156, 476, 231, 332, 9, 360, 361, 419, 416, 360, 318, 153, 92, 235, 274, 476], [107, 128, 215, 139, 216, 146, 112, 111, 230, 37, 454, 7, 4, 455, 112, 218, 301, 360, 189, 416, 269, 468, 119, 476, 350, 85, 476], [54, 128, 282, 328, 200, 138, 464, 434, 146, 92, 260, 403, 370, 360, 289, 416, 261, 475, 476]]\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "x = vocab.index('UNKNOWN')\n",
    "wordToIndex = [vocab.index(df.iloc[0, df.columns.get_loc('Word')])]\n",
    "        \n",
    "for i in xrange(1, no_of_datapoints):\n",
    "    word = df.iloc[i, df.columns.get_loc('Word')]\n",
    "\n",
    "    if df['Sentence #'].isnull()[i] == True:\n",
    "        if word in vocab:\n",
    "            wordToIndex.append(vocab.index(word))\n",
    "        else:\n",
    "            wordToIndex.append(x)\n",
    "    else: \n",
    "        sentences.append(wordToIndex)\n",
    "        wordToIndex = []\n",
    "        if word in vocab:\n",
    "            wordToIndex.append(vocab.index(word))\n",
    "        else:\n",
    "            wordToIndex.append(x)\n",
    "\n",
    "print sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('sentenceIndex.txt', 'wb') as fp:\n",
    "    pickle.dump(sentences, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open ('sentenceIndex.txt', 'rb') as fp:\n",
    "    sentences = pickle.load(fp)\n",
    "\n",
    "print sentences\n",
    "print len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42, 40)\n"
     ]
    }
   ],
   "source": [
    "sentencesPadded = pad_sequences(sentences)\n",
    "print sentencesPadded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "\n",
    "wordPretrainedModel = gensim.models.Word2Vec.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "\n",
    "wordEmbeddingPretrainedMatrix = np.zeros((len(vocab), EMBEDDING_DIM))\n",
    "\n",
    "for i in xrange(0, len(vocab)):\n",
    "    if word in \n",
    "    wordEmbeddingPretrainedMatrix[i] = (model.wv[vocab[i]])\n",
    "\n",
    "print wordEmbeddingPretrainedMatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "[[0 0 0 ..., 0 0 1]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " [0 0 0 ..., 0 0 1]]\n",
      "(42,)\n"
     ]
    }
   ],
   "source": [
    "EntityOneHot = pd.get_dummies(df.iloc[:no_of_datapoints,df.columns.get_loc('Tag')])\n",
    "\n",
    "labels = np.empty((EntityOneHot.shape), dtype = int)\n",
    "\n",
    "for i in xrange(0,EntityOneHot.shape[1]):\n",
    "    numbers = np.array(EntityOneHot.iloc[:,i])\n",
    "    labels[:,i] = numbers\n",
    "              \n",
    "print len(labels)\n",
    "print labels\n",
    "\n",
    "labels_sequence_list = []\n",
    "labels_sequence = [labels[0]]\n",
    "        \n",
    "for i in xrange(1, no_of_datapoints):\n",
    "    if df['Sentence #'].isnull()[i] == True:\n",
    "            labels_sequence.append(labels[i])\n",
    "    else: \n",
    "        labels_sequence_list.append(labels_sequence)\n",
    "        labels_sequence = []\n",
    "        labels_sequence.append(labels[i])\n",
    "\n",
    "labels_sequence_list = np.array(labels_sequence_list)\n",
    "print labels_sequence_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42, 40, 11)\n"
     ]
    }
   ],
   "source": [
    "labels_sequence_list_padded = pad_sequences(labels_sequence_list)\n",
    "print labels_sequence_list_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "print len(charactersList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 40, 17)\n",
      "(None, 40, 17, 30)\n",
      "(None, 40, 128)\n",
      "(None, 40, 300)\n",
      "(None, 40, 428)\n",
      "(None, 40, 256)\n",
      "(None, 40, 256)\n",
      "(None, 40, 11)\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "main_input = Input(shape = (sentence_character_list_padded.shape[1], sentence_character_list_padded.shape[2],),\n",
    "                   dtype = 'float32', name = 'main_input')\n",
    "print main_input._keras_shape\n",
    "\n",
    "character_embedding_intermediate = Embedding(input_dim = len(charactersList), output_dim = 30, \n",
    "                                input_length = (sentence_character_list_padded.shape[1], sentence_character_list_padded.shape[2])) (main_input)\n",
    "print character_embedding_intermediate._keras_shape\n",
    "\n",
    "LSTM_character_forward = TimeDistributed(LSTM(64, return_sequences = False, go_backwards = False))(character_embedding_intermediate)\n",
    "LSTM_character_backward = TimeDistributed(LSTM(64, return_sequences = False, go_backwards = True))(character_embedding_intermediate)\n",
    "character_embedding = keras.layers.concatenate([LSTM_character_forward, LSTM_character_backward])\n",
    "print character_embedding._keras_shape\n",
    "\n",
    "auxillary_input = Input(shape = (sentencesPadded.shape[1],), dtype = 'float32', name = 'second_input')\n",
    "word_pretrained_embedding = Embedding(input_dim = len(vocab), output_dim = EMBEDDING_DIM, \n",
    "                                      input_length = sentencesPadded.shape[1])(auxillary_input)\n",
    "#                                      weights = [wordEmbeddingPretrainedMatrix], trainable = True)  \n",
    "print word_pretrained_embedding._keras_shape  \n",
    "\n",
    "word_embedding = keras.layers.concatenate([word_pretrained_embedding, character_embedding])\n",
    "print word_embedding._keras_shape\n",
    "\n",
    "LSTM_out_forward = LSTM(128, go_backwards = False, return_sequences = True)(word_embedding)\n",
    "LSTM_out_backward = LSTM(128, go_backwards = True, return_sequences = True)(word_embedding)\n",
    "LSTM_out = keras.layers.concatenate([LSTM_out_forward, LSTM_out_backward])\n",
    "print LSTM_out._keras_shape\n",
    "\n",
    "dense_out_1 = Dense(256, activation = 'relu')(LSTM_out)\n",
    "print dense_out_1._keras_shape\n",
    "\n",
    "dropout_1 = Dropout(0.4)(dense_out_1)\n",
    "\n",
    "main_output = Dense(labels.shape[1], activation = 'softmax', name = 'main_output')(dropout_1)\n",
    "print main_output._keras_shape\n",
    "\n",
    "model = Model(inputs = [main_input, auxillary_input], outputs = [main_output])\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 40, 17)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)          (None, 40, 17, 30)    2970        main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "second_input (InputLayer)        (None, 40)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistribu (None, 40, 64)        24320       embedding_9[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistrib (None, 40, 64)        24320       embedding_9[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)         (None, 40, 300)       143100      second_input[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)     (None, 40, 128)       0           time_distributed_9[0][0]         \n",
      "                                                                   time_distributed_10[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)     (None, 40, 428)       0           embedding_10[0][0]               \n",
      "                                                                   concatenate_13[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "lstm_19 (LSTM)                   (None, 40, 128)       285184      concatenate_14[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "lstm_20 (LSTM)                   (None, 40, 128)       285184      concatenate_14[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)     (None, 40, 256)       0           lstm_19[0][0]                    \n",
      "                                                                   lstm_20[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 40, 256)       65792       concatenate_15[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 40, 256)       0           dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "main_output (Dense)              (None, 40, 11)        2827        dropout_5[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 833,697\n",
      "Trainable params: 833,697\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class Metrics(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        predict = np.asarray(self.model.predict([self.validation_data[0], self.validation_data[1]]))\n",
    "        \n",
    "        argmax = np.argmax(predict, axis = 2)\n",
    "        \n",
    "        predictions = np.zeros((predict.shape[0], predict.shape[1], predict.shape[2]), dtype = int)\n",
    "        for i in xrange(0,argmax.shape[0]):\n",
    "            for j in xrange(0, argmax.shape[1]):\n",
    "                predictions[i][j][argmax[i][j]] = 1\n",
    "                \n",
    "        predictions = predictions.reshape((-1,predict.shape[2]))\n",
    "\n",
    "        targ = self.validation_data[2]\n",
    "        targ = targ.reshape((-1,predict.shape[2]))\n",
    "\n",
    "        f1s = f1_score(targ, predictions, average = 'micro')\n",
    "        print \"f1_score = \", f1s\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32 samples, validate on 10 samples\n",
      "Epoch 1/3\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.7203 - acc: 0.4726 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bf1_score =  0.694444444444\n",
      "32/32 [==============================] - 17s - loss: 0.7077 - acc: 0.4742 - val_loss: 0.3430 - val_acc: 0.5625\n",
      "Epoch 2/3\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.4492 - acc: 0.4919 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bf1_score =  0.694444444444\n",
      "32/32 [==============================] - 15s - loss: 0.4547 - acc: 0.4898 - val_loss: 0.3362 - val_acc: 0.5625\n",
      "Epoch 3/3\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.4364 - acc: 0.4935 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bf1_score =  0.694444444444\n",
      "32/32 [==============================] - 15s - loss: 0.4234 - acc: 0.4898 - val_loss: 0.3354 - val_acc: 0.5625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17478b10>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = Metrics()\n",
    "x = 32\n",
    "#model.fit([sentence_character_list_padded, sentencesPadded],[labels_sequence_list_padded], epochs = 3, batch_size = 1)\n",
    "model.fit([sentence_character_list_padded[:x], sentencesPadded[:x]],[labels_sequence_list_padded[:x]], \n",
    "          validation_data = [[sentence_character_list_padded[x:], sentencesPadded[x:]],labels_sequence_list_padded[x:]],\n",
    "          epochs = 3, batch_size = 1, callbacks = [metrics])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
